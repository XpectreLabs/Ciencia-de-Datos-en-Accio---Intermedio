# **M칩dulo 5: Feature Engineering y Optimizaci칩n de Modelos (4 horas)**

## 游꿢**Objetivos Espec칤ficos**
1. **Feature Engineering**: Crear y transformar caracter칤sticas (variables) de manera eficiente para mejorar el rendimiento de los modelos.
2. **Selecci칩n y Reducci칩n de Dimensionalidad**: Seleccionar las variables m치s relevantes y/o reducir la dimensionalidad utilizando t칠cnicas como PCA u otros m칠todos avanzados.
3. **Optimizaci칩n de Hiperpar치metros**: Ajustar y optimizar los hiperpar치metros de un modelo (e.g., Grid Search, Random Search) para obtener la mejor configuraci칩n posible.
4. **Evaluaci칩n de Modelos**: Evaluar y comparar distintas configuraciones de modelos para seleccionar la m치s adecuada en funci칩n del problema.

---

## **Tema 5.1: Feature Engineering**

### **Concepto y Ubicaci칩n en el Flujo de Trabajo**

**쯈u칠 es Feature Engineering?**  
Feature Engineering es el proceso de crear, transformar y seleccionar caracter칤sticas o variables dentro de un dataset con el fin de mejorar la capacidad predictiva y el rendimiento general de los modelos de Machine Learning. Es una de las etapas m치s cr칤ticas en el flujo de trabajo de Machine Learning, ya que influye directamente en la calidad de los datos utilizados para entrenar los modelos.

Este proceso se lleva a cabo dentro del **preprocesamiento de datos**, antes de entrenar el modelo, y puede involucrar diversas tareas como la creaci칩n de nuevas variables, la transformaci칩n de las existentes o la eliminaci칩n de aquellas que no contribuyen al modelo.

### **Importancia de Feature Engineering**
- **Mejora significativa del rendimiento**: Las variables bien dise침adas pueden proporcionar al modelo informaci칩n m치s relevante y 칰til, lo que se traduce en un mejor rendimiento, con m칠tricas m치s altas como la precisi칩n y el recall.
  
- **Captura de relaciones complejas**: Muchas veces, las relaciones entre las variables y la variable objetivo son complejas o no lineales. Feature Engineering permite representar estas relaciones de una manera que los modelos pueden entender.

- **Reducci칩n de ruido**: Al seleccionar o transformar las variables adecuadas, se puede reducir el impacto de ruido o informaci칩n irrelevante en el modelo, ayudando a evitar el sobreajuste (overfitting) y mejorando la generalizaci칩n del modelo.

- **Mejora de la interpretabilidad del modelo**: Las variables transformadas o creadas correctamente pueden hacer que el modelo sea m치s f치cil de interpretar, ayudando a entender los factores m치s importantes que afectan las predicciones.

#### **Ejemplo de impacto**:  
Supongamos que tenemos un dataset de ventas con variables como "ventas" y "precio_unitario". Si creamos una nueva variable "ingreso_total" multiplicando `ventas` por `precio_unitario`, esta nueva caracter칤stica puede ofrecer una representaci칩n m치s precisa del rendimiento de las ventas, ayudando al modelo a identificar patrones de ingresos que no eran evidentes cuando trat치bamos a `ventas` y `precio_unitario` por separado.

---

### **Otras Razones para Realizar Feature Engineering**
- **Preparaci칩n para Modelos Complejos**: En el caso de algoritmos que no manejan bien caracter칤sticas categ칩ricas o no lineales, el proceso de ingenier칤a de caracter칤sticas puede preparar el dataset para obtener mejores resultados.

- **Adaptabilidad a diferentes tipos de datos**: Feature Engineering tambi칠n permite adaptar los modelos a diferentes tipos de datos, como datos temporales, geoespaciales, o de texto, transform치ndolos en un formato adecuado para que los modelos puedan aprender de ellos.

- **Reducci칩n de dimensionalidad**: A trav칠s de la creaci칩n de nuevas caracter칤sticas o la selecci칩n de las m치s relevantes, es posible reducir la dimensionalidad de los datos, lo que ayuda a mejorar el rendimiento de los modelos, especialmente en situaciones con grandes vol칰menes de datos.

### **츼reas Comunes de Feature Engineering**
1. **Transformaci칩n de Datos**: Involucra aplicar funciones matem치ticas a las caracter칤sticas, como la normalizaci칩n, la estandarizaci칩n, o la creaci칩n de logaritmos de las variables.
   
2. **Creaci칩n de Caracter칤sticas Derivadas**: Se generan nuevas variables a partir de las existentes, como combinaciones entre variables o variables agregadas que capturan informaci칩n relevante.

3. **Manejo de Variables Categ칩ricas**: Convertir las variables categ칩ricas a un formato adecuado (por ejemplo, One-Hot Encoding, Label Encoding) para que puedan ser utilizadas por modelos que no manejan directamente datos no num칠ricos.

4. **Manejo de Valores Faltantes**: Implica estrategias como la imputaci칩n de valores faltantes para asegurar que el modelo no se vea afectado por datos incompletos.

5. **Detecci칩n y Manejo de Outliers**: Identificar y tratar los valores at칤picos (outliers) para evitar que distorsionen las predicciones del modelo.

6. **Variables Temporales**: Descomponer y extraer informaci칩n 칰til de las variables temporales, como fechas, horas o ciclos estacionales.

---

### **Creaci칩n de Nuevas Variables**

#### **1. Transformaciones Matem치ticas**
- Aplicar transformaciones para estabilizar la varianza o capturar relaciones no lineales.
- Ayudar a que los datos sigan una distribuci칩n m치s adecuada para los modelos.
- Mejorar el rendimiento de los modelos lineales en situaciones de relaciones no lineales.
- Minimizar el impacto de los valores at칤picos o extremos.
- Facilitar la convergencia de algunos algoritmos de Machine Learning.
- Adaptar los datos a una escala m치s uniforme, mejorando la estabilidad num칠rica.
- Permitir la mejor representaci칩n de los datos cuando hay variaciones grandes en los valores.
- **쮺u치ndo Aplicar Transformaciones Matem치ticas?**
  - **Sesgo de Distribuci칩n**: Cuando las variables tienen distribuciones sesgadas que afectan negativamente el modelo.
  - **Relaciones No Lineales**: Cuando se quiere transformar la relaci칩n entre las caracter칤sticas y la variable objetivo de no lineal a lineal.
  - **Escalado de Datos**: Cuando las caracter칤sticas tienen diferentes rangos de magnitud y deben ser normalizadas o estandarizadas.

- **Ejemplo**:
```python
import numpy as np
df['log_ventas'] = np.log(df['ventas'] + 1)  # Evitar log(0)

```

#### **2. Interacciones entre Variables**
- Las interacciones entre variables consisten en crear nuevas columnas combinando variables existentes mediante operaciones matem치ticas como la multiplicaci칩n, divisi칩n, resta o suma. Esto ayuda a capturar relaciones complejas entre las caracter칤sticas y puede mejorar significativamente el rendimiento del modelo.
- Permiten identificar patrones ocultos que no son evidentes a simple vista y que pueden influir de manera importante en la predicci칩n.
- Es 칰til cuando se sospecha que el efecto de una variable depende del valor de otra variable.
- Ayuda a mejorar la precisi칩n de modelos que no logran captar la relaci칩n entre variables de manera independiente.
- Ejemplo pr치ctico: Si tienes una variable `edad` y `ingreso`, la interacci칩n de ambas podr칤a representar una nueva variable como `edad_por_ingreso`, lo que podr칤a reflejar mejor ciertos patrones.
- Tambi칠n puede evitar la multicolinealidad entre variables independientes, si la combinaci칩n de estas da lugar a nuevas variables con menos redundancia.

- **Ejemplo:**
```python
df['ventas_x_precio'] = df['ventas'] * df['precio_unitario']
```
#### **3. Variables Categ칩ricas**

**One-Hot Encoding**
- Este m칠todo convierte cada categor칤a en una columna binaria. Es 칰til cuando las categor칤as no tienen un orden inherente, ya que evita introducir un sesgo en el modelo. Cada columna resultante tiene un valor de 0 o 1 dependiendo de si el registro pertenece a esa categor칤a.
- One-Hot Encoding es adecuado para modelos que no pueden manejar datos categ칩ricos directamente, como los modelos lineales o los 치rboles de decisi칩n.
- La principal ventaja es que no impone ninguna relaci칩n ordinal entre las categor칤as, lo cual es esencial para evitar suposiciones incorrectas en el modelo.
- Sin embargo, puede aumentar significativamente la dimensionalidad del dataset cuando hay muchas categor칤as, lo que puede hacer que el modelo sea m치s costoso computacionalmente y m치s propenso al sobreajuste.
- **Ejemplo**: Si tienes una columna `color` con valores ["rojo", "verde", "azul"], One-Hot Encoding generar치 tres columnas: `color_rojo`, `color_verde` y `color_azul`. Un valor "rojo" se representar칤a como [1, 0, 0], "verde" como [0, 1, 0] y "azul" como [0, 0, 1].
- **Ventajas de One-Hot Encoding**:
  - Evita introducir sesgos en el modelo relacionados con el orden de las categor칤as.
  - Es 칰til para categor칤as nominales sin ninguna relaci칩n jer치rquica o secuencial.
- **Desventajas de One-Hot Encoding**:
  - Aumenta la dimensionalidad del conjunto de datos, lo que puede llevar a una mayor complejidad computacional.
  - Puede causar problemas con modelos que no manejan bien la alta dimensionalidad.
- **Ejemplo de c칩digo:**
```python
# Convierte una columna categ칩rica en m칰ltiples columnas binarias
pd.get_dummies(df['region'], prefix='region')
```

**Label Encoding**
- En este enfoque, cada categor칤a se asigna un valor num칠rico 칰nico. Es adecuado cuando las categor칤as tienen un orden inherente o se desea reducir la dimensionalidad.
- A diferencia de One-Hot Encoding, Label Encoding introduce una relaci칩n ordinal entre las categor칤as, lo que puede ser 칰til si las categor칤as tienen un orden natural (por ejemplo, "bajo", "medio", "alto").
- **Desventaja**: Si las categor칤as no tienen un orden l칩gico, Label Encoding puede introducir sesgos en el modelo, ya que los valores num칠ricos asignados podr칤an ser interpretados como representaciones de magnitudes, lo cual no tiene sentido en algunas situaciones.
- **Ejemplo**: Si tienes una columna `tama침o` con valores ["peque침o", "mediano", "grande"], Label Encoding los transformar치 en [0, 1, 2], representando el orden impl칤cito.
- **Ventajas de Label Encoding**:
  - No aumenta la dimensionalidad del conjunto de datos.
  - Es m치s eficiente computacionalmente para un n칰mero grande de categor칤as.
- **Desventajas de Label Encoding**:
  - Introduce un orden entre las categor칤as que puede ser inapropiado cuando no existe una relaci칩n ordinal entre ellas.
```python
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
df['region_encoded'] = encoder.fit_transform(df['region'])
```

#### **4. Variables Temporales**

- **Extracci칩n de componentes temporales**: Las variables temporales derivadas de fechas y horas pueden proporcionar informaci칩n valiosa en muchos modelos predictivos. Al extraer componentes clave como el "mes", "d칤a de la semana", "hora" o "a침o", se puede identificar patrones que est치n relacionados con el ciclo temporal.
  
- **Comportamiento estacional y tendencias**: El comportamiento de los datos puede cambiar seg칰n la temporada o la hora del d칤a. Por ejemplo, el comportamiento de los consumidores puede ser diferente durante los d칤as laborales frente a los fines de semana, o durante los meses festivos frente a los meses normales. Las variables temporales ayudan a capturar estos efectos.

- **Tendencias a largo plazo**: Adem치s de los patrones estacionales y semanales, las variables temporales tambi칠n pueden ayudar a identificar tendencias a largo plazo. Por ejemplo, la venta de ciertos productos podr칤a seguir una tendencia creciente o decreciente con el paso de los a침os.

- **Variables temporales y machine learning**: Las caracter칤sticas derivadas de fechas y tiempos son esenciales en 치reas como la predicci칩n de series de tiempo, la planificaci칩n de la demanda, y el an치lisis de eventos. Permiten que los modelos aprendan no solo de las relaciones directas entre las caracter칤sticas y la variable objetivo, sino tambi칠n de los patrones temporales que pueden influir en la predicci칩n.

**Ventajas de las Variables Temporales:**
  - **Captura de patrones estacionales**: Ayuda a detectar comportamientos peri칩dicos en los datos, como variaciones mensuales o semanales, lo que puede mejorar las predicciones en muchos tipos de modelos.
  - **Mejora en el rendimiento predictivo**: La inclusi칩n de informaci칩n temporal relevante puede mejorar la capacidad predictiva de los modelos, ya que muchas veces los comportamientos est치n influenciados por la hora del d칤a, el mes del a침o o eventos recurrentes como los festivos.
    
**Desventajas de las Variables Temporales:**
  - **Dimensionalidad adicional**: La extracci칩n de m칰ltiples componentes temporales puede aumentar la dimensionalidad de los datos, lo que podr칤a afectar el rendimiento del modelo si no se manejan adecuadamente.
  - **Complejidad en la interpretaci칩n**: La relaci칩n entre las variables temporales y la variable objetivo no siempre es lineal, lo que podr칤a hacer que la interpretaci칩n de los resultados sea m치s compleja y requiera t칠cnicas de modelado m치s sofisticadas.
  - **Necesidad de cuidado en el preprocesamiento**: Las fechas pueden requerir un manejo especial, especialmente si hay datos faltantes, inconsistencias o errores en las fechas. Esto puede requerir un preprocesamiento adicional antes de utilizarlas en el modelo.
  
**Ejemplo:**
```python
df['mes'] = df['fecha'].dt.month
df['es_festivo'] = df['fecha'].apply(lambda x: x in lista_festivos)
```

#### Ejemplo 5.1
-[**`Ejemplo 5.1`**](Ejemplo5_1.ipynb)

#### Ejercicio 5.1
-[**`Ejercicio Feature Engineering`**](Ejercicio5_1.ipynb)

---

## Tema 5.2: Selecci칩n de Caracter칤sticas y Reducci칩n de Dimensionalidad

### **Selecci칩n de Caracter칤sticas (Feature Selection)**

La **selecci칩n de caracter칤sticas** es un proceso esencial en el preprocesamiento de datos que busca identificar las caracter칤sticas m치s relevantes para un modelo, eliminando aquellas que son redundantes o irrelevantes. Este paso puede mejorar el rendimiento del modelo, reducir la complejidad computacional y facilitar la interpretaci칩n de los resultados.

#### **Problema de Alta Dimensionalidad**
- **Rendimiento computacional reducido**: A medida que el n칰mero de caracter칤sticas aumenta, los algoritmos de aprendizaje pueden volverse m치s lentos y requerir mayores recursos computacionales. Esto tambi칠n puede aumentar el tiempo de entrenamiento y el riesgo de sobreajuste.
  
- **Dificultad para interpretar los modelos**: Los modelos con muchas variables son m치s dif칤ciles de interpretar, lo que puede dificultar la comprensi칩n de c칩mo las caracter칤sticas est치n influyendo en las predicciones. En modelos complejos como las redes neuronales, esto se vuelve especialmente problem치tico.
  
- **Aumento del riesgo de sobreajuste (overfitting)**: Con demasiadas caracter칤sticas, el modelo puede ajustarse demasiado a los datos de entrenamiento, perdiendo capacidad para generalizar a nuevos datos.

#### **M칠todos de Selecci칩n de Caracter칤sticas**
Existen varios enfoques para la selecci칩n de caracter칤sticas, que se pueden agrupar en tres categor칤as: **basados en filtros**, **basados en envolventes (wrappers)** e **integrados**.

---

### **M칠todos Basados en Filtros**
Estos m칠todos eval칰an cada caracter칤stica de forma independiente y la seleccionan o rechazan en funci칩n de su relevancia para el problema. No requieren un modelo de aprendizaje para ser aplicados.

- **Correlaci칩n**: Este m칠todo eval칰a la relaci칩n entre las variables y la variable objetivo utilizando m칠tricas como el coeficiente de correlaci칩n de Pearson para caracter칤sticas num칠ricas. Se eliminan caracter칤sticas altamente correlacionadas entre s칤 para reducir redundancia.
  
- **Test chi-cuadrado para variables categ칩ricas**: Este test estad칤stico eval칰a la dependencia entre dos variables categ칩ricas. Si una variable categ칩rica no tiene una relaci칩n significativa con la variable objetivo, puede ser descartada.

#### **Ventajas**:
- R치pido y computacionalmente eficiente.
- No requiere entrenamiento de un modelo.
- Facilita la identificaci칩n de caracter칤sticas irrelevantes r치pidamente.

#### **Desventajas**:
- No tiene en cuenta las interacciones entre las caracter칤sticas.
- Puede no ser tan preciso en problemas complejos donde las interacciones entre variables son importantes.

#### **Ejemplo:**
```python
from sklearn.feature_selection import chi2
chi_scores = chi2(X, y)
```

---

### **M칠todos Basados en Wrappers**
Los m칠todos de selecci칩n basada en envolventes usan un modelo de aprendizaje para evaluar la importancia de las caracter칤sticas. Se seleccionan las caracter칤sticas seg칰n su rendimiento en un modelo y se repite el proceso iterativamente.

- **Recursive Feature Elimination (RFE)**: RFE es un enfoque iterativo que elimina las caracter칤sticas menos importantes seg칰n el rendimiento del modelo. En cada iteraci칩n, el modelo se entrena con las caracter칤sticas restantes, y se elimina la caracter칤stica menos relevante. Este proceso se repite hasta que se obtiene el conjunto 칩ptimo de caracter칤sticas.

#### **Ventajas**:
- Toma en cuenta la interacci칩n entre las caracter칤sticas.
- A menudo produce mejores resultados, ya que considera el rendimiento del modelo directamente.

#### **Desventajas**:
- Computacionalmente costoso, especialmente con muchos datos y caracter칤sticas.
- Requiere entrenamiento de un modelo, lo que puede ser lento.

#### **Ejemplo:**
```python
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
selector = RFE(model, n_features_to_select=5)
selector.fit(X, y)
```


---

### **M칠todos Integrados (Embedded)**
Los m칠todos integrados seleccionan caracter칤sticas durante el proceso de entrenamiento del modelo. Esto significa que la selecci칩n de caracter칤sticas se realiza autom치ticamente al mismo tiempo que el modelo aprende. Algunos algoritmos de machine learning tienen mecanismos integrados de selecci칩n de caracter칤sticas.

- **Regularizaci칩n (Lasso y Ridge)**:  
  - **Lasso (L1 regularization)**: Penaliza los coeficientes de las caracter칤sticas para reducir algunos de ellos a cero, eliminando caracter칤sticas irrelevantes. Es particularmente 칰til cuando hay muchas variables correlacionadas.
  - **Ridge (L2 regularization)**: Penaliza los coeficientes, pero no los elimina completamente, lo que permite manejar multicolinealidad sin reducir las caracter칤sticas a cero.
  
- **Modelos de 츼rboles (como Random Forest y Gradient Boosting)**:  
  Los modelos basados en 치rboles tienen la capacidad de asignar una "importancia" a cada caracter칤stica, basada en la mejora que produce en el rendimiento del modelo al dividir los datos. Las caracter칤sticas menos importantes pueden ser eliminadas seg칰n esta medida de importancia.

#### **Ventajas**:
- Tienen en cuenta las interacciones entre caracter칤sticas de manera eficiente.
- Suelen ser m치s eficientes computacionalmente que los m칠todos de envolvente.

#### **Desventajas**:
- Puede ser m치s dif칤cil de interpretar en comparaci칩n con los m칠todos basados en filtros.
- Dependiente del modelo utilizado, puede no ser aplicable a todos los algoritmos.

```python
from sklearn.linear_model import Lasso
model = Lasso(alpha=0.1)
model.fit(X, y)
print(model.coef_)
```

---

### **Reducci칩n de Dimensionalidad**

La **reducci칩n de dimensionalidad** es el proceso de reducir el n칰mero de caracter칤sticas (variables) en un conjunto de datos mientras se preserva la mayor cantidad de informaci칩n posible. Este proceso es esencial para simplificar modelos, mejorar la eficiencia computacional y reducir el riesgo de sobreajuste.

#### **쯇or qu칠 es importante?**
- **Reduce la complejidad computacional**: Menos caracter칤sticas implican menor tiempo de procesamiento y menor memoria requerida.
- **Mejora la visualizaci칩n**: Al reducir las dimensiones a 2 o 3, es posible visualizar los datos, lo que facilita la interpretaci칩n y el an치lisis.
- **Reduce el riesgo de sobreajuste**: Menos caracter칤sticas pueden ayudar a evitar que el modelo se ajuste demasiado a los datos de entrenamiento.
- **Mejora la interpretabilidad**: Un modelo con menos caracter칤sticas es m치s f치cil de entender y analizar.

#### **PCA (Principal Component Analysis)**

**PCA** es uno de los m칠todos m치s comunes para la reducci칩n de dimensionalidad. Su objetivo es transformar un conjunto de variables correlacionadas en un conjunto de **componentes principales** que son lineales, no correlacionados y que capturan la mayor parte de la varianza de los datos.

##### **쮺칩mo funciona PCA?**
1. **Estandarizaci칩n**: Los datos deben ser estandarizados para que todas las variables tengan la misma escala. Esto es crucial cuando las caracter칤sticas tienen diferentes unidades de medida (por ejemplo, peso en kilogramos y altura en metros).
  
2. **C치lculo de la matriz de covarianza**: Se calcula la matriz de covarianza entre las variables, que describe c칩mo se relacionan entre s칤.

3. **Obtenci칩n de los vectores propios y valores propios**: Los vectores propios representan las direcciones de mayor varianza en los datos, y los valores propios indican la cantidad de varianza que cada componente captura.

4. **Selecci칩n de los componentes principales**: Se seleccionan los componentes principales seg칰n la varianza que explican. Generalmente, se seleccionan los primeros componentes que explican un porcentaje significativo de la varianza, como el 80% o m치s, dependiendo de los requisitos del modelo.

5. **Proyecci칩n de los datos en el nuevo espacio**: Los datos originales se proyectan sobre los componentes principales seleccionados, reduciendo la dimensionalidad.

##### **Ventajas de PCA**:
- **Reducci칩n de dimensionalidad**: Disminuye el n칰mero de variables manteniendo la mayor parte de la informaci칩n.
- **Mejora del rendimiento del modelo**: Reduce la complejidad, lo que puede mejorar la precisi칩n y reducir el tiempo de entrenamiento del modelo.
- **Mejora la visualizaci칩n**: Al reducir las dimensiones a 2 o 3, es m치s f치cil visualizar los datos.
  
##### **Desventajas de PCA**:
- **Dificultad de interpretaci칩n**: Los componentes principales son combinaciones lineales de las variables originales, lo que puede hacer que sea dif칤cil interpretar los resultados directamente.
- **P칠rdida de informaci칩n**: Aunque PCA trata de retener la mayor varianza, siempre hay una peque침a p칠rdida de informaci칩n al reducir las dimensiones.

##### **쮺u치ndo usar PCA?**
- Cuando tienes un conjunto de datos con muchas caracter칤sticas y deseas reducir la complejidad.
- Cuando las caracter칤sticas est치n altamente correlacionadas.
- En modelos que pueden beneficiarse de la reducci칩n de la dimensionalidad sin perder mucha informaci칩n.

#### **Otras T칠cnicas de Reducci칩n de Dimensionalidad**
- **t-SNE (t-Distributed Stochastic Neighbor Embedding)**: Es una t칠cnica no lineal que es 칰til para la visualizaci칩n de datos de alta dimensionalidad, pero no necesariamente preserva la varianza global.
- **LDA (Linear Discriminant Analysis)**: A diferencia de PCA, que es una t칠cnica no supervisada, LDA se utiliza principalmente para tareas de clasificaci칩n, buscando reducir la dimensionalidad mientras mantiene la informaci칩n que ayuda a separar las clases.

#### **Ejemplo**
```python
  from sklearn.decomposition import PCA
  pca = PCA(n_components=2)
  X_pca = pca.fit_transform(X)
  print("Varianza explicada:", pca.explained_variance_ratio_)
  ```

#### **Visualizaci칩n de PCA:**
```python
import matplotlib.pyplot as plt
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)
plt.title("Visualizaci칩n PCA")
plt.show()
```

#### Ejemplo 5.2
-[**`Ejemplo 5.2`**](ejemplo5.2/Ejemplo5_2.ipynb)

#### Ejercicio 5.2
-[**`Ejercicio Selecci칩n de Caracter칤sticas y Reducci칩n de Dimensionalidad`**](ejercicio5.2/Ejercicio5_2.ipynb)

---

### **Tema 5.3: Optimizaci칩n de Modelos (Hyperparameter Tuning)**

La optimizaci칩n de modelos, tambi칠n conocida como **ajuste de hiperpar치metros**, es un proceso crucial para mejorar el rendimiento de un modelo de machine learning. Los hiperpar치metros son configuraciones que determinan la estructura del modelo y c칩mo se entrena, pero no se ajustan durante el entrenamiento en s칤.

#### **Introducci칩n a la Optimizaci칩n de Hiperpar치metros**

Los **hiperpar치metros** pueden tener un gran impacto en el rendimiento de un modelo. Ajustarlos adecuadamente permite que el modelo se adapte mejor a los datos y, por lo tanto, generalice mejor a datos no vistos. El ajuste de hiperpar치metros se realiza antes del entrenamiento y afecta aspectos como la complejidad del modelo, la velocidad de convergencia y la capacidad de generalizaci칩n.

##### **Par치metros vs Hiperpar치metros**  
- **Par치metros**: Son los valores que se ajustan durante el entrenamiento del modelo. En un modelo de regresi칩n, por ejemplo, los **coeficientes** de las variables son par치metros. El modelo aprende estos valores a partir de los datos.
  
- **Hiperpar치metros**: Son valores configurados antes de que comience el entrenamiento y determinan el comportamiento del modelo. Ejemplos incluyen:
  - **Profundidad de un 치rbol** en un modelo de 치rbol de decisi칩n.
  - **N칰mero de vecinos (k)** en un clasificador K-Nearest Neighbors (K-NN).
  - **Tasa de aprendizaje** en algoritmos de optimizaci칩n como el descenso por gradiente.
  
#### **M칠todos de B칰squeda de Hiperpar치metros**

Existen diferentes t칠cnicas para encontrar los mejores hiperpar치metros. Las dos m치s comunes son **Grid Search** y **Random Search**.

##### **Grid Search (B칰squeda en Rejilla)**

**Grid Search** realiza una b칰squeda exhaustiva, probando todas las combinaciones posibles de hiperpar치metros dentro de un espacio predefinido. Esta t칠cnica garantiza que se consideren todas las combinaciones posibles, lo que puede ser muy 칰til, pero tambi칠n puede ser **computacionalmente costosa**.

###### **Ventajas**:
- Garantiza encontrar la mejor combinaci칩n de hiperpar치metros dentro del espacio definido.
- M칠todo sistem치tico y exhaustivo.

###### **Desventajas**:
- **Costoso en tiempo**: Si el n칰mero de hiperpar치metros y sus valores es grande, la b칰squeda puede llevar mucho tiempo y recursos computacionales.
- No es pr치ctico para modelos con muchos hiperpar치metros o para grandes datasets.

###### **Ejemplo**:
Sup칩n que tienes dos hiperpar치metros: la profundidad de un 치rbol (`max_depth`) y el n칰mero de 치rboles (`n_estimators`). Si `max_depth` puede tomar 3 valores (1, 2, 3) y `n_estimators` 2 valores (50, 100), Grid Search probar칤a todas las combinaciones posibles (1x2 = 6 combinaciones).

###### **Ejemplo c칩digo**:
```python
from sklearn.model_selection import GridSearchCV
param_grid = {'max_depth': [3, 5, 10], 'n_estimators': [50, 100]}
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
grid_search.fit(X, y)
```

##### **Random Search (B칰squeda Aleatoria)**

**Random Search** selecciona aleatoriamente un subconjunto de combinaciones de hiperpar치metros dentro de un rango definido. Aunque no garantiza que se encuentren las mejores combinaciones, puede ser **mucho m치s eficiente** en t칠rminos de tiempo computacional cuando se exploran muchos hiperpar치metros.

###### **Ventajas**:
- **Menor tiempo de computaci칩n**: Al probar aleatoriamente solo un subconjunto de combinaciones, suele ser m치s r치pido que Grid Search.
- **Menos costoso computacionalmente**: Requiere menos evaluaciones, por lo que es m치s eficiente cuando los hiperpar치metros tienen un espacio de b칰squeda grande.

###### **Desventajas**:
- **No garantiza encontrar la mejor combinaci칩n**: Puede que no se exploren todas las opciones posibles.
- **Dependencia del n칰mero de iteraciones**: La calidad de los resultados depende de cu치ntas iteraciones se realicen.

###### **Ejemplo**:
Si usas Random Search para los mismos hiperpar치metros que en el ejemplo anterior, podr칤as seleccionar aleatoriamente 3 combinaciones de `max_depth` y `n_estimators`, en lugar de probar todas las combinaciones posibles.

###### **Ejemplo c칩digo**:
```python
from sklearn.model_selection import RandomizedSearchCV
random_search = RandomizedSearchCV(RandomForestClassifier(), param_distributions=param_grid, n_iter=10, cv=5)
random_search.fit(X, y)
```

#### **T칠cnicas Avanzadas de Optimizaci칩n**

Existen m칠todos m치s avanzados para la optimizaci칩n de hiperpar치metros que son m치s eficientes que Grid y Random Search:

##### **1. Bayesian Optimization**
La **optimizaci칩n bayesiana** utiliza un enfoque probabil칤stico para explorar los hiperpar치metros, modelando la funci칩n de p칠rdida y optimiz치ndola de manera iterativa. Esta t칠cnica es m치s eficiente, ya que no prueba todas las combinaciones, sino que prioriza las combinaciones que tienen m치s probabilidades de mejorar el rendimiento.

###### **Ventajas**:
- Menos iteraciones que Grid y Random Search.
- M치s eficiente y r치pido en t칠rminos de b칰squeda de hiperpar치metros.

##### **2. Optimizaci칩n por Enjambre de Part칤culas (PSO)**
El algoritmo de **optimizaci칩n por enjambre de part칤culas** es una t칠cnica inspirada en la naturaleza (espec칤ficamente en el comportamiento de los enjambres de aves) que busca la mejor soluci칩n en el espacio de hiperpar치metros a trav칠s de un proceso de b칰squeda distribuido.

###### **Ventajas**:
- Puede encontrar soluciones globales de manera eficiente en problemas complejos.
- Es especialmente 칰til en problemas con muchas variables no lineales.

##### **3. Algoritmos Evolutivos**
Los **algoritmos evolutivos** son t칠cnicas inspiradas en la evoluci칩n biol칩gica, donde se realizan mutaciones y cruces entre un conjunto de soluciones para generar nuevas combinaciones de hiperpar치metros. Esto permite explorar el espacio de soluciones de manera efectiva.

###### **Ventajas**:
- Capaz de manejar grandes espacios de b칰squeda de hiperpar치metros.
- Buena capacidad para evitar quedar atrapado en m칤nimos locales.

#### Ejemplo 5.3
-[**`Ejemplo 5.3`**](ejemplo5.3/Ejemplo5_3.ipynb)

#### Ejercicio 5.3
-[**`Ejercicio Optimizaci칩n de Hiperpar치metros con Random Forest`**](ejercicio5.3/Ejercicio5_3.ipynb)

---

### **Secci칩n 5.4: Evaluaci칩n, Comparaci칩n y Selecci칩n de Modelos**

La evaluaci칩n, comparaci칩n y selecci칩n de modelos son pasos fundamentales para asegurar que el modelo final sea el m치s adecuado para el problema en cuesti칩n. Esto implica no solo evaluar el rendimiento del modelo, sino tambi칠n analizar su capacidad para generalizar a nuevos datos, su interpretabilidad y la importancia de las caracter칤sticas utilizadas en la predicci칩n.

#### **Comparaci칩n de Distintas Estrategias**
La comparaci칩n de diferentes estrategias y enfoques es esencial para determinar la efectividad de un modelo y sus ajustes. Algunas estrategias clave incluyen:

- **Evaluar el modelo antes y despu칠s de la ingenier칤a de caracter칤sticas**:
  - La ingenier칤a de caracter칤sticas puede mejorar significativamente el rendimiento de un modelo. Evaluar c칩mo cambia el rendimiento antes y despu칠s de realizar transformaciones o combinaciones de caracter칤sticas (como la creaci칩n de nuevas variables, normalizaci칩n o eliminaci칩n de caracter칤sticas irrelevantes) permite observar el impacto directo en la precisi칩n del modelo.
  
- **Comparar resultados del modelo sin optimizaci칩n vs con optimizaci칩n de hiperpar치metros**:
  - A menudo, un modelo inicial puede ser 칰til, pero no alcanza su m치ximo potencial hasta que se optimizan los hiperpar치metros. Comparar el rendimiento de un modelo antes de aplicar t칠cnicas de optimizaci칩n de hiperpar치metros (como Grid Search o Random Search) y despu칠s de aplicar esas t칠cnicas puede proporcionar informaci칩n importante sobre la efectividad de la b칰squeda de hiperpar치metros y el ajuste de los par치metros del modelo.

#### **Curva de Aprendizaje (Learning Curve)**
La curva de aprendizaje es una herramienta valiosa para evaluar el rendimiento de un modelo mientras se entrena con diferentes tama침os de datos. Este an치lisis es crucial para entender c칩mo un modelo puede mejorar con m치s datos o ajustes en el entrenamiento.

- **An치lisis de sesgo y varianza con relaci칩n al tama침o del conjunto de entrenamiento**:
  - **Sesgo**: Si la curva muestra un alto error en los datos de entrenamiento y prueba, esto sugiere un sesgo alto, lo que significa que el modelo est치 subajustado (underfitting).
  - **Varianza**: Si el modelo tiene un buen rendimiento en los datos de entrenamiento pero un bajo rendimiento en los datos de prueba, indica alta varianza, lo que sugiere sobreajuste (overfitting).
  - El an치lisis de la curva de aprendizaje permite identificar si el modelo est치 aprendiendo correctamente y si se necesita ajustar el tama침o del conjunto de datos o utilizar m치s regularizaci칩n.

- **Interpretaci칩n de resultados: 쮼l modelo necesita m치s datos, m치s regularizaci칩n?**:
  - **Si el modelo muestra sobreajuste (overfitting)**: Se pueden a침adir m치s datos de entrenamiento, aplicar t칠cnicas de regularizaci칩n (como **Lasso** o **Ridge**) o utilizar m칠todos como el **dropout** en redes neuronales.
  - **Si el modelo muestra subajuste (underfitting)**: Se pueden incluir m치s caracter칤sticas, ajustar los hiperpar치metros o cambiar a un modelo m치s complejo.

#### **Interpretabilidad de Modelos**
La interpretabilidad es crucial cuando se quiere entender c칩mo un modelo toma decisiones, especialmente en aplicaciones sensibles como la medicina, la finanza o el derecho.

- **Importancia de Variables (Feature Importances) en Modelos Tipo 츼rbol**:
  - En modelos basados en 치rboles como **Random Forest** o **XGBoost**, la importancia de las caracter칤sticas se calcula evaluando cu치nto contribuye cada variable a la reducci칩n de la impureza en cada divisi칩n del 치rbol. Esto ayuda a identificar qu칠 caracter칤sticas tienen m치s influencia en las predicciones del modelo y puede ser 칰til para la selecci칩n de caracter칤sticas o para la interpretaci칩n del modelo.
  
- **M칠todos como SHAP o LIME para Modelos M치s Complejos (Opcional)**:
  - **SHAP (SHapley Additive exPlanations)**: Proporciona explicaciones detalladas sobre c칩mo cada caracter칤stica contribuye a una predicci칩n espec칤fica. Es una extensi칩n de la teor칤a de juegos y proporciona una forma robusta y precisa de descomponer las predicciones de un modelo en t칠rminos de sus caracter칤sticas.
  
  - **LIME (Local Interpretable Model-Agnostic Explanations)**: Permite interpretar modelos complejos (como redes neuronales) de manera local, analizando el modelo alrededor de una instancia espec칤fica. LIME genera un modelo m치s simple y comprensible para explicar c칩mo el modelo complejo llega a su decisi칩n en ese caso particular.

  Estos m칠todos permiten que incluso los modelos m치s complejos, como redes neuronales profundas o modelos de ensemble, sean m치s comprensibles y accesibles para los usuarios, mejorando la confianza en las predicciones del modelo.

###### **Ejemplo c칩digo**:
```python
from sklearn.model_selection import learning_curve
train_sizes, train_scores, valid_scores = learning_curve(RandomForestClassifier(), X, y, cv=5)
plt.plot(train_sizes, train_scores.mean(axis=1), label='Entrenamiento')
plt.plot(train_sizes, valid_scores.mean(axis=1), label='Validaci칩n')
plt.legend()
plt.show()
```

#### Ejemplo 5.4
-[**`Ejemplo 5.4`**](ejemplo5.4/Ejemplo5_4.ipynb)

#### Ejercicio 5.4
-[**`Ejercicio Evaluaci칩n y Comparaci칩n de Modelos`**](ejercicio5.4/Ejercicio5_4.ipynb)

#### Actividad Final
-[**`Actividad Final`**](actividad_final/Actividad_Final5.ipynb)
